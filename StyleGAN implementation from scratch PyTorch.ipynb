{"metadata":{"kernelspec":{"display_name":"Python 3.10.8 ('phd')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"20682def01f3f467eb74e11ca1f7fb8aba52d6795e0e97b11e537403717040cb"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn, optim\nfrom torchvision import datasets, transforms\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import save_image\nfrom math import log2\nimport numpy as np\nimport os\nfrom tqdm import tqdm\nimport matplotlib.pylab as plt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATSET = '/kaggle/input/women-clothes'\nSTART_TRAIN_IMG_SIZE = 4\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nLR = 1e-3\nBATCH_SIZES = [256,256,128,64,32,16]\nCHANNELS_IMG = 3\nZ_DIm = 512\nW_DIM = 512\nIN_CHANNELS = 512\nLAMBDA_GP = 10\nPROGRESSIVE_EPOCHS = [30] * len(BATCH_SIZES)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_loader(image_size):\n    trainsform = transforms.Compose(\n        [transforms.Resize((image_size, image_size)),\n         transforms.ToTensor(),\n         transforms.RandomHorizontalFlip(p=0.5),\n         transforms.Normalize(\n            [0.5 for _ in range(CHANNELS_IMG)],\n            [0.5 for _ in range(CHANNELS_IMG)],\n         )\n        ]\n    )\n    batch_size = BATCH_SIZES[int(log2(image_size/4))]\n    dataset = datasets.ImageFolder(root=DATSET, transform=trainsform)\n    loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=True\n    )\n    return loader, dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_loader():\n    loader,_ = get_loader(128)\n    cloth,_  = next(iter(loader))\n    _,ax     = plt.subplots(3,3,figsize=(8,8))\n    plt.suptitle('Some real samples')\n    ind = 0\n    for k in range(3):\n        for kk in range(3):\n            ax[k][kk].imshow((cloth[ind].permute(1,2,0)+1)/2)\n            ind +=1\ncheck_loader() ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Noice apping network\n# AdaIN\n# Progressice growing\nfactors = [1,1,1,1/2,1/4,1/8,1/16,1/32]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WSLinear(nn.Module):\n    def __init__(\n        self, in_features, out_features\n    ):\n        super(WSLinear,self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.scale  = (2/in_features) ** 0.5\n        self.bias   = self.linear.bias\n        self.linear.bias = None\n\n        nn.init.normal_(self.linear.weight)\n        nn.init.zeros_(self.bias)\n\n    def forward(self,x):\n        return self.linear(x * self.scale) + self.bias","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PixenNorm(nn.Module):\n    def __init__(self):\n        super(PixenNorm, self).__init__()\n        self.epsilon = 1e-8\n    def forward(self,x ):\n        return x / torch.sqrt(torch.mean(x**2, dim=1, keepdim=True)+  self.epsilon)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MappingNetwork(nn.Module):\n    def __init__(self, z_dim, w_dim):\n        super().__init__()\n        self.mapping = nn.Sequential(\n            PixenNorm(),\n            WSLinear(z_dim, w_dim),\n            nn.ReLU(),\n            WSLinear(w_dim, w_dim),\n            nn.ReLU(),\n            WSLinear(w_dim, w_dim),\n            nn.ReLU(),\n            WSLinear(w_dim, w_dim),\n            nn.ReLU(),\n            WSLinear(w_dim, w_dim),\n            nn.ReLU(),\n            WSLinear(w_dim, w_dim),\n            nn.ReLU(),\n            WSLinear(w_dim, w_dim),\n            nn.ReLU(),\n            WSLinear(w_dim, w_dim),\n        )\n    \n    def forward(self,x):\n        return self.mapping(x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AdaIN(nn.Module):\n    def __init__(self, channels, w_dim):\n        super().__init__()\n        self.instance_norm = nn.InstanceNorm2d(channels)\n        self.style_scale   = WSLinear(w_dim, channels)\n        self.style_bias    = WSLinear(w_dim, channels)\n\n    def forward(self,x,w):\n        x = self.instance_norm(x)\n        style_scale = self.style_scale(w).unsqueeze(2).unsqueeze(3)\n        style_bias  = self.style_bias(w).unsqueeze(2).unsqueeze(3)\n        return style_scale * x + style_bias","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class injectNoise(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.weight = nn.Parameter(torch.zeros(1,channels,1,1))\n\n    def forward(self, x):\n        noise = torch.randn((x.shape[0], 1, x.shape[2], x.shape[3]), device = x.device)\n        return x + self.weight + noise","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GenBlock(nn.Module):\n    def __init__(self, in_channel, out_channel, w_dim):\n        super(GenBlock, self).__init__()\n        self.conv1 = WSConv2d(in_channel, out_channel)\n        self.conv2 = WSConv2d(out_channel, out_channel)\n        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n        self.inject_noise1 = injectNoise(out_channel)\n        self.inject_noise2 = injectNoise(out_channel)\n        self.adain1 = AdaIN(out_channel, w_dim)\n        self.adain2 = AdaIN(out_channel, w_dim)\n    def forward(self, x,w):\n        x = self.adain1(self.leaky(self.inject_noise1(self.conv1(x))), w)\n        x = self.adain2(self.leaky(self.inject_noise2(self.conv2(x))), w)\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, z_dim, w_dim, in_channels, img_channels=3):\n        super().__init__()\n        self.starting_cte = nn.Parameter(torch.ones(1, in_channels, 4,4))\n        self.map = MappingNetwork(z_dim, w_dim)\n        self.initial_adain1 = AdaIN(in_channels, w_dim)\n        self.initial_adain2 = AdaIN(in_channels, w_dim)\n        self.initial_noise1 = injectNoise(in_channels)\n        self.initial_noise2 = injectNoise(in_channels)\n        self.initial_conv   = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n        self.leaky          = nn.LeakyReLU(0.2, inplace=True)\n\n        self.initial_rgb    = WSConv2d(\n            in_channels, img_channels, kernel_size = 1, stride=1, padding=0\n        )\n        self.prog_blocks, self.rgb_layers = (\n            nn.ModuleList([]),\n            nn.ModuleList([self.initial_rgb])\n        )\n\n        for i in range(len(factors)-1):\n            conv_in_c  = int(in_channels * factors[i])\n            conv_out_c = int(in_channels * factors[i+1])\n            self.prog_blocks.append(GenBlock(conv_in_c, conv_out_c, w_dim))\n            self.rgb_layers.append(WSConv2d(conv_out_c, img_channels, kernel_size = 1, stride=1, padding=0))\n        \n    def fade_in(self, alpha, upscaled, generated):\n        return torch.tanh(alpha * generated + (1-alpha ) * upscaled)\n\n    def forward(self, noise, alpha, steps):\n        w = self.map(noise)\n        x = self.initial_adain1(self.initial_noise1(self.starting_cte),w)\n        x = self.initial_conv(x)\n        out = self.initial_adain2(self.leaky(self.initial_noise2(x)), w)\n\n        if steps == 0:\n            return self.initial_rgb(x)\n        \n        for step in range(steps):\n            upscaled = F.interpolate(out, scale_factor=2, mode = 'bilinear')\n            out      = self.prog_blocks[step](upscaled,w)\n\n        final_upscaled = self.rgb_layers[steps-1](upscaled)\n        final_out      = self.rgb_layers[steps](out)\n\n        return self.fade_in(alpha, final_upscaled, final_out)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WSConv2d(nn.Module):\n    def __init__(\n        self, in_channels, out_channels, kernel_size=3, stride=1, padding=1\n    ):\n        super(WSConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        self.scale = (2 / (in_channels * (kernel_size ** 2))) ** 0.5\n        self.bias = self.conv.bias\n        self.conv.bias = None\n\n        # initialize conv layer\n        nn.init.normal_(self.conv.weight)\n        nn.init.zeros_(self.bias)\n\n    def forward(self, x):\n        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ConvBlock, self).__init__()\n        self.conv1 = WSConv2d(in_channels, out_channels)\n        self.conv2 = WSConv2d(out_channels, out_channels)\n        self.leaky = nn.LeakyReLU(0.2)\n\n    def forward(self, x):\n        x = self.leaky(self.conv1(x))\n        x = self.leaky(self.conv2(x))\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, in_channels, img_channels=3):\n        super(Discriminator, self).__init__()\n        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n        self.leaky = nn.LeakyReLU(0.2)\n\n        # here we work back ways from factors because the discriminator\n        # should be mirrored from the generator. So the first prog_block and\n        # rgb layer we append will work for input size 1024x1024, then 512->256-> etc\n        for i in range(len(factors) - 1, 0, -1):\n            conv_in = int(in_channels * factors[i])\n            conv_out = int(in_channels * factors[i - 1])\n            self.prog_blocks.append(ConvBlock(conv_in, conv_out))\n            self.rgb_layers.append(\n                WSConv2d(img_channels, conv_in, kernel_size=1, stride=1, padding=0)\n            )\n\n        # perhaps confusing name \"initial_rgb\" this is just the RGB layer for 4x4 input size\n        # did this to \"mirror\" the generator initial_rgb\n        self.initial_rgb = WSConv2d(\n            img_channels, in_channels, kernel_size=1, stride=1, padding=0\n        )\n        self.rgb_layers.append(self.initial_rgb)\n        self.avg_pool = nn.AvgPool2d(\n            kernel_size=2, stride=2\n        )  # down sampling using avg pool\n\n        # this is the block for 4x4 input size\n        self.final_block = nn.Sequential(\n            # +1 to in_channels because we concatenate from MiniBatch std\n            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n            nn.LeakyReLU(0.2),\n            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n            nn.LeakyReLU(0.2),\n            WSConv2d(\n                in_channels, 1, kernel_size=1, padding=0, stride=1\n            ),  # we use this instead of linear layer\n        )\n\n    def fade_in(self, alpha, downscaled, out):\n        \"\"\"Used to fade in downscaled using avg pooling and output from CNN\"\"\"\n        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n        return alpha * out + (1 - alpha) * downscaled\n\n    def minibatch_std(self, x):\n        batch_statistics = (\n            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n        )\n        # we take the std for each example (across all channels, and pixels) then we repeat it\n        # for a single channel and concatenate it with the image. In this way the discriminator\n        # will get information about the variation in the batch/image\n        return torch.cat([x, batch_statistics], dim=1)\n\n    def forward(self, x, alpha, steps):\n        # where we should start in the list of prog_blocks, maybe a bit confusing but\n        # the last is for the 4x4. So example let's say steps=1, then we should start\n        # at the second to last because input_size will be 8x8. If steps==0 we just\n        # use the final block\n        cur_step = len(self.prog_blocks) - steps\n\n        # convert from rgb as initial step, this will depend on\n        # the image size (each will have it's on rgb layer)\n        out = self.leaky(self.rgb_layers[cur_step](x))\n\n        if steps == 0:  # i.e, image is 4x4\n            out = self.minibatch_std(out)\n            return self.final_block(out).view(out.shape[0], -1)\n\n        # because prog_blocks might change the channels, for down scale we use rgb_layer\n        # from previous/smaller size which in our case correlates to +1 in the indexing\n        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n        out = self.avg_pool(self.prog_blocks[cur_step](out))\n\n        # the fade_in is done first between the downscaled and the input\n        # this is opposite from the generator\n        out = self.fade_in(alpha, downscaled, out)\n\n        for step in range(cur_step + 1, len(self.prog_blocks)):\n            out = self.prog_blocks[step](out)\n            out = self.avg_pool(out)\n\n        out = self.minibatch_std(out)\n        return self.final_block(out).view(out.shape[0], -1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_examples(gen, steps, n=100):\n\n    gen.eval()\n    alpha = 1.0\n    for i in range(n):\n        with torch.no_grad():\n            noise = torch.randn(1, Z_DIM).to(DEVICE)\n            img = gen(noise, alpha, steps)\n            if not os.path.exists(f'saved_examples/step{steps}'):\n                os.makedirs(f'saved_examples/step{steps}')\n            save_image(img*0.5+0.5, f\"saved_examples/step{steps}/img_{i}.png\")\n    gen.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n    BATCH_SIZE, C, H, W = real.shape\n    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n    interpolated_images = real * beta + fake.detach() * (1 - beta)\n    interpolated_images.requires_grad_(True)\n\n    # Calculate critic scores\n    mixed_scores = critic(interpolated_images, alpha, train_step)\n \n    # Take the gradient of the scores with respect to the images\n    gradient = torch.autograd.grad(\n        inputs=interpolated_images,\n        outputs=mixed_scores,\n        grad_outputs=torch.ones_like(mixed_scores),\n        create_graph=True,\n        retain_graph=True,\n    )[0]\n    gradient = gradient.view(gradient.shape[0], -1)\n    gradient_norm = gradient.norm(2, dim=1)\n    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n    return gradient_penalty","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(\n    critic,\n    gen,\n    loader,\n    dataset,\n    step,\n    alpha,\n    opt_critic,\n    opt_gen\n):\n    loop = tqdm(loader, leave=True)\n\n    for batch_idx, (real, _) in enumerate(loop):\n        real = real.to(DEVICE)\n        cur_batch_size = real.shape[0]\n        noise = torch.randn(cur_batch_size, Z_DIm).to(DEVICE)\n        fake  = gen(noise, alpha, step)\n        critic_real = critic(real, alpha, step)\n        critic_fake = critic(fake.detach(), alpha, step)\n        gp = gradient_penalty(critic, real, fake, alpha, step, DEVICE)\n        loss_critic = (\n            -(torch.mean(critic_real) - torch.mean(critic_fake))\n            + LAMBDA_GP * gp\n            + (0.001) * torch.mean(critic_real ** 2)\n        )\n\n        critic.zero_grad()\n        loss_critic.backward()\n        opt_critic.step()\n\n        gen_fake = critic(fake, alpha, step)\n        loss_gen = -torch.mean(gen_fake)\n\n        gen.zero_grad()\n        loss_gen.backward()\n        opt_gen.step()\n\n        alpha += cur_batch_size / (\n            PROGRESSIVE_EPOCHS[step] * 0.5 * len(dataset)\n        )\n        alpha = min(alpha,1)\n\n\n        loop.set_postfix(\n            gp = gp.item(),\n            loss_critic = loss_critic.item()\n        )\n    return alpha","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen = Generator(\n    Z_DIm, W_DIM, IN_CHANNELS, CHANNELS_IMG\n).to(DEVICE)\ncritic = Discriminator(IN_CHANNELS, CHANNELS_IMG).to(DEVICE)\nopt_gen = optim.Adam([{'params': [param for name, param in gen.named_parameters() if 'map' not in name]},\n                     {'params': gen.map.parameters(), 'lr': 1e-5}], lr=LR, betas =(0.0, 0.99))\nopt_critic = optim.Adam(\n    critic.parameters(), lr= LR, betas =(0.0, 0.99)\n)\n\ngen.train()\ncritic.train()\nstep = int(log2(START_TRAIN_IMG_SIZE / 4))\nfor num_epochs in PROGRESSIVE_EPOCHS[step:]:\n    alpha = 1e-7\n    loader, dataset = get_loader(4*2**step)\n    print('Curent image size: '+str(4*2**step))\n\n    for epoch in range(num_epochs):\n        print(f'Epoch [{epoch + 1}/ {num_epochs}')\n        alpha = train_fn(\n            critic, gen, loader, dataset, step, alpha, opt_critic, opt_gen\n        )\n    generate_examples(gen, step)\n    step +=1\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}